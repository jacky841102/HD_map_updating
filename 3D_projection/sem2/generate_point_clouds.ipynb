{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Argoverse HD map and COLMAP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms import affine_matrix_from_points\n",
    "from argoverse.data_loading.argoverse_tracking_loader import ArgoverseTrackingLoader\n",
    "from scipy.spatial import transform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from visualize_30hz_benchmark_data_on_map import DatasetOnMapVisualizer\n",
    "from argoverse.map_representation.map_api import ArgoverseMap\n",
    "import logging\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "# path to the argoverse dataset\n",
    "tracking_dataset_dir = '../argoverse-tracking/sample/'\n",
    "\n",
    "# log_index determines which log in the argoverse dataset directory to use\n",
    "log_index = 0\n",
    "\n",
    "# load argoverse HD map\n",
    "argoverse_loader = ArgoverseTrackingLoader(tracking_dataset_dir)\n",
    "log_id = argoverse_loader.log_list[log_index]\n",
    "argoverse_data = argoverse_loader[log_index]\n",
    "\n",
    "am = ArgoverseMap()\n",
    "city_name = argoverse_data.city_name\n",
    "dataset_dir = tracking_dataset_dir\n",
    "experiment_prefix = 'visualization_demo'\n",
    "use_existing_files = True\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "domv = DatasetOnMapVisualizer(dataset_dir, experiment_prefix, use_existing_files=use_existing_files, log_id=argoverse_data.current_log)\n",
    "\n",
    "# colmap_output_dir points to the results from COLMAP\n",
    "colmap_output_dir = 'reconstruction/sparse/' + log_id + '/'\n",
    "# detection results directory\n",
    "detection_output_dir = './focal_detection/'\n",
    "# where to save computed traffic sign locations\n",
    "projection_output_dir = './'\n",
    "\n",
    "\n",
    "# flags that determine which ring cameras to use\n",
    "use_fc, use_fl, use_sl, use_rl, use_rr, use_sr, use_fr = [1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "#read in colmap results\n",
    "poses_colmap = dict()   #dict that queries camera poses with image name\n",
    "points_colmap = dict()  #dict that queries colmap feature points IDs with image name\n",
    "id2name_colmap = dict() #dict that queries image name with colmap image IDs\n",
    "points3D = dict()       #dict that queries colmap points' 3D coordinates with colmap point IDs\n",
    "\n",
    "with open(colmap_output_dir + 'images.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "for i,line in enumerate(lines):\n",
    "    if line[0] == '#':\n",
    "        continue\n",
    "    else:\n",
    "        if i % 2 == 0:\n",
    "            fields = line.split(' ')\n",
    "            image_name = fields[-1]\n",
    "            image_id = int(fields[0])\n",
    "            quat = fields[1:5]\n",
    "            quatanion = np.array([float(q) for q in quat])\n",
    "            trans = fields[5:8]\n",
    "            translation = np.array([float(t) for t in trans])\n",
    "            camera_id = int(fields[8])\n",
    "            poses_colmap[image_name] = [quatanion, translation, image_id, camera_id]\n",
    "            id2name_colmap[image_name] = image_id\n",
    "        else:\n",
    "            fields = line.split(' ')\n",
    "            points_2d = np.array([float(pt) for pt in fields])\n",
    "            points_colmap[image_name] = points_2d\n",
    "            \n",
    "with open(colmap_output_dir + 'points3D.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "for i,line in enumerate(lines):\n",
    "    if line[0] == '#':\n",
    "        continue\n",
    "    else: \n",
    "            fields = line.split(' ')\n",
    "            points_attr = np.array([float(pt) for pt in fields])\n",
    "            points3D[points_attr[0]] = np.array([points_attr[1],points_attr[2],points_attr[3]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the 3D point clouds for a specifc timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ring_front_center_315969692021102752.jpg\n",
      "(3, 17728)\n",
      "(17728,)\n",
      "(17728, 3)\n",
      "(17728, 3)\n"
     ]
    }
   ],
   "source": [
    "# read in argoverse data\n",
    "import numpy.linalg as LA\n",
    "\n",
    "calib = argoverse_data.get_calibration(\"ring_front_center\")\n",
    "\n",
    "# Change this to the timestamp you want, idx from [0, 155]\n",
    "idx = 80\n",
    "\n",
    "\n",
    "city_to_egovehicle_se3 = argoverse_data.get_pose(idx)    \n",
    "img_fc = argoverse_data.image_list_sync['ring_front_center'][idx]\n",
    "img_fc = img_fc.split('/')\n",
    "print(img_fc[-1])\n",
    "image = plt.imread(tracking_dataset_dir + log_id + '/ring_front_center/' + img_fc[-1])\n",
    "\n",
    "pc = argoverse_data.get_lidar(idx)\n",
    "uv = calib.project_ego_to_image(pc).T\n",
    "\n",
    "idx_ = np.where(np.logical_and.reduce((uv[0, :] >= 0.0, uv[0, :] < np.shape(image)[1] - 1.0,\n",
    "                                                      uv[1, :] >= 0.0, uv[1, :] < np.shape(image)[0] - 1.0,\n",
    "                                                      uv[2, :] > 0)))\n",
    "idx_ = idx_[0]\n",
    "uv1 =uv[:, idx_]\n",
    "print(uv1.shape)\n",
    "rgb = []\n",
    "u = uv1[1, :]\n",
    "print(u.shape)\n",
    "v = uv1[0, :]\n",
    "for i in range(uv1.shape[1]):\n",
    "    rgb.append(image[int(round(u[i])),int(round(v[i])),:])\n",
    "    \n",
    "rgb = np.array(rgb)\n",
    "print(rgb.shape)\n",
    "pc = calib.project_image_to_ego(uv1.T)\n",
    "pc = city_to_egovehicle_se3.transform_point_cloud(pc)\n",
    "print(pc.shape)\n",
    "import pandas as pd\n",
    "from pyntcloud import PyntCloud\n",
    "d = {'x': pc[:,0],'y': pc[:,1],'z': pc[:,2], \n",
    "             'red' : rgb[:,0], 'green' : rgb[:,1], 'blue' : rgb[:,2]}\n",
    "        \n",
    "cloud = PyntCloud(pd.DataFrame(data=d))\n",
    "cloud.to_file(\"./output.ply\")\n",
    "import scipy.io\n",
    "scipy.io.savemat('test.mat', dict(pc=pc,color=rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
