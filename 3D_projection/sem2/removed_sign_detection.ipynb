{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Argoverse HD map and COLMAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:argoverse.data_loading.synchronization_database:Building SynchronizationDB\n",
      "INFO:root:syncronizing camera and lidar sensor...\n",
      "INFO:argoverse.data_loading.vector_map_loader:Loaded root: ArgoverseVectorMap\n",
      "INFO:argoverse.data_loading.vector_map_loader:Loaded root: ArgoverseVectorMap\n"
     ]
    }
   ],
   "source": [
    "from transforms import affine_matrix_from_points\n",
    "from argoverse.data_loading.argoverse_tracking_loader import ArgoverseTrackingLoader\n",
    "from scipy.spatial import transform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from visualize_30hz_benchmark_data_on_map import DatasetOnMapVisualizer\n",
    "from argoverse.map_representation.map_api import ArgoverseMap\n",
    "import logging\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "# path to the argoverse dataset\n",
    "tracking_dataset_dir = '../argoverse-tracking/sample/'\n",
    "\n",
    "# log_index determines which log in the argoverse dataset directory to use\n",
    "log_index = 0\n",
    "\n",
    "# load argoverse HD map\n",
    "argoverse_loader = ArgoverseTrackingLoader(tracking_dataset_dir)\n",
    "log_id = argoverse_loader.log_list[log_index]\n",
    "argoverse_data = argoverse_loader[log_index]\n",
    "\n",
    "am = ArgoverseMap()\n",
    "city_name = argoverse_data.city_name\n",
    "dataset_dir = tracking_dataset_dir\n",
    "experiment_prefix = 'visualization_demo'\n",
    "use_existing_files = True\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "domv = DatasetOnMapVisualizer(dataset_dir, experiment_prefix, use_existing_files=use_existing_files, log_id=argoverse_data.current_log)\n",
    "\n",
    "# colmap_output_dir points to the results from COLMAP\n",
    "colmap_output_dir = 'reconstruction/sparse/' + log_id + '/'\n",
    "# detection results directory\n",
    "detection_output_dir = './focal_detection/'\n",
    "# where to save computed traffic sign locations\n",
    "projection_output_dir = './'\n",
    "\n",
    "\n",
    "#read in colmap results\n",
    "poses_colmap = dict()   #dict that queries camera poses with image name\n",
    "points_colmap = dict()  #dict that queries colmap feature points IDs with image name\n",
    "id2name_colmap = dict() #dict that queries image name with colmap image IDs\n",
    "points3D = dict()       #dict that queries colmap points' 3D coordinates with colmap point IDs\n",
    "\n",
    "with open(colmap_output_dir + 'images.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "for i,line in enumerate(lines):\n",
    "    if line[0] == '#':\n",
    "        continue\n",
    "    else:\n",
    "        if i % 2 == 0:\n",
    "            fields = line.split(' ')\n",
    "            image_name = fields[-1]\n",
    "            image_id = int(fields[0])\n",
    "            quat = fields[1:5]\n",
    "            quatanion = np.array([float(q) for q in quat])\n",
    "            trans = fields[5:8]\n",
    "            translation = np.array([float(t) for t in trans])\n",
    "            camera_id = int(fields[8])\n",
    "            poses_colmap[image_name] = [quatanion, translation, image_id, camera_id]\n",
    "            id2name_colmap[image_name] = image_id\n",
    "        else:\n",
    "            fields = line.split(' ')\n",
    "            points_2d = np.array([float(pt) for pt in fields])\n",
    "            points_colmap[image_name] = points_2d\n",
    "            \n",
    "with open(colmap_output_dir + 'points3D.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "for i,line in enumerate(lines):\n",
    "    if line[0] == '#':\n",
    "        continue\n",
    "    else: \n",
    "            fields = line.split(' ')\n",
    "            points_attr = np.array([float(pt) for pt in fields])\n",
    "            points3D[points_attr[0]] = np.array([points_attr[1],points_attr[2],points_attr[3]])\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in traffic sign detection results and pre-stored traffic sign positions from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# read in detection results\n",
    "boxes = np.load(detection_output_dir + log_id + '.npz',allow_pickle=True)\n",
    "boxes_images_list = [full_name.split('/')[-1] for full_name in boxes['input_image_names']]   \n",
    "order = np.argsort(boxes_images_list)\n",
    "boxes_images_list = np.array(boxes_images_list)[order]    # image names for the detection bounding boxes\n",
    "boxes_boxes_list = boxes['bbox_list'][order]              # detection bounding boxes\n",
    "boxes_scores_list = boxes['score_list'][order]            # confidence scores for the detection bounding boxes\n",
    "boxes_class_list = boxes['pred_classes_list'][order]      # detected categories for the detection bounding boxes\n",
    "\n",
    "# read in stored traffic sign postions and the 3D-to-3D similarity transformation\n",
    "database = np.load(log_id + '_focal_projected.npz',allow_pickle=True)\n",
    "argo_sign_poses = database['argo_sign_poses']\n",
    "categories = database['categories']\n",
    "colmap2argo = database['colmap2argo']\n",
    "argo2colmap = database['argo2colmap']\n",
    "M = colmap2argo\n",
    "M_inv = argo2colmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inside(x1,y1,x2,y2,x,y):\n",
    "    return x1 <= x and x <= x2 and y1 <= y and y <=y2\n",
    "\n",
    "# discard these signs that are expected too far away from the vehicle\n",
    "def checkDistance(image_name, sign_position):\n",
    "    q = poses_colmap[image_name][0]\n",
    "    quat = [q[1], q[2], q[3], q[0]]\n",
    "    R = transform.Rotation.from_quat(quat)\n",
    "    t = poses_colmap[image_name][1]\n",
    "    vehicle_position_colmap = - R.as_matrix().T.dot(t)\n",
    "    vehicle_position_argo = M.dot(np.append(vehicle_position_colmap, [1]))\n",
    "    L2_distance = np.sqrt(np.sum(np.square(vehicle_position_argo[:3] - sign_position)))\n",
    "    return L2_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each traffic sign stored in the databse, get its expected 2D postion in each frame and count the total number\n",
    "of times it has been successfully detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3. 86. 25. 20. 39. 10. 36.  1. 31.  5.]\n"
     ]
    }
   ],
   "source": [
    "# store the number of successful detections for each prestored traffic sign in the database\n",
    "number_of_occurences = np.zeros(argo_sign_poses.shape[0])   \n",
    "\n",
    "calib = argoverse_data.get_calibration(\"ring_front_center\")\n",
    "\n",
    "for idx in range(len(argoverse_data.image_list_sync['ring_front_center'])):\n",
    "    city_to_egovehicle_se3 = argoverse_data.get_pose(idx)\n",
    "    \n",
    "    # compute the 3D-to-2D projection from HD map coordinate to image view\n",
    "    img_fc = argoverse_data.image_list_sync['ring_front_center'][idx]\n",
    "    img_fc = img_fc.split('/')\n",
    "    image_name = img_fc[-2] + '/' + img_fc[-1]\n",
    "    q = poses_colmap[image_name][0]\n",
    "    quat = [q[1], q[2], q[3], q[0]]\n",
    "    R = transform.Rotation.from_quat(quat)\n",
    "    t = poses_colmap[image_name][1]\n",
    "    E = np.zeros((3,4))\n",
    "    E[:3,:3] = R.as_matrix()\n",
    "    E[:3,3] = t\n",
    "    \n",
    "    # project each pre-stored sign in the database\n",
    "    for i in range(argo_sign_poses.shape[0]):\n",
    "        # Start projection only when the vehicle is close enough to the sign \n",
    "        distance_2_sign = checkDistance(image_name, argo_sign_poses[i,:])\n",
    "        if distance_2_sign > 50:\n",
    "            continue\n",
    "        pts_argo = np.append(argo_sign_poses[i,:], [1]).reshape(4,1)\n",
    "        pts_colmap = M_inv @ pts_argo\n",
    "        pts_uv = E @ pts_colmap\n",
    "        K = np.array([[1352.38, 0, 960], [0, 1352.38, 600], [0, 0, 1]]) #camera intrinsics\n",
    "        uv = K @ pts_uv\n",
    "        uv = uv/uv[2]\n",
    "        uv = np.squeeze(uv)\n",
    "        \n",
    "        # check whether the position in the image view lies within any detection bounding box of that category\n",
    "        detection_idx = np.where(boxes_images_list==img_fc[-1])[0].item()\n",
    "        for j,box in enumerate(boxes_boxes_list[detection_idx]):\n",
    "            margin = 5       # The width that we want to expand each bounxing boxes with\n",
    "            x1 = int(box[0]) # - margin\n",
    "            y1 = int(box[1]) # - margin\n",
    "            x2 = int(box[2]) # + margin\n",
    "            y2 = int(box[3]) # + margin\n",
    "            if check_inside(x1, y1, x2, y2, uv[0], uv[1]) and boxes_class_list[detection_idx][j]==categories[i]:\n",
    "                number_of_occurences[i] += 1\n",
    "\n",
    "print(number_of_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the old signs that are not detected in the sequnce, and update the npz file accrodingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = number_of_occurences != 0\n",
    "argo_sign_poses = argo_sign_poses[valid]\n",
    "categories = categories[valid]\n",
    "\n",
    "np.savez(projection_output_dir + log_id+'_after_removal.npz', \n",
    "         argo_sign_poses=argo_sign_poses, \n",
    "         categories=categories,\n",
    "         colmap2argo=colmap2argo,\n",
    "         argo2colmap=argo2colmap,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
